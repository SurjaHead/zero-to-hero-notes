{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4949363-0575-41f8-bcb8-8d8cba385161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-05 10:48:52--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M  2.41MB/s    in 0.4s    \n",
      "\n",
      "2024-11-05 10:48:53 (2.41 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f575bc92-d74f-4ab0-8780-60814ffa8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fcf8893-c6fe-4155-b1ab-890468d20fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ae72cb-920c-4331-ad01-6544669e3db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e36215-fca0-45a8-8ce0-41a3c1c1df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text))) # set removes duplicate items, then convereted to a list, thens orted\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcb35a4-9b51-41e2-995e-89c34607b901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder takes a string and outputs a list  of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4f2d1-9477-48e8-a6d3-a216f037d44f",
   "metadata": {},
   "source": [
    "NOTE: Typically, people use sub-word level tokenizers (SentenceGram from Google and Tiktoken form OpenAI), but to keep it simple, we'll use a character level tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f51a583-9d8d-4691-844d-403898adbd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encoding the entire text\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdcb5519-72b1-435e-9373-a603fa71d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "n = int(0.9*len(data)) # first 90% will be train\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e997a1e7-1347-417b-b684-60af84b9d6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54c582-56b0-4f16-9800-ff3a37dc90a8",
   "metadata": {},
   "source": [
    "NOTE: We use block_size + 1 above because when we feed it into the transformer, it's always with a length of context + 1. Here's how the first few examples go:\n",
    "- (18 -> 47)\n",
    "- (18, 47) -> (56)\n",
    "- (18, 47, 56) -> (57)\n",
    "- ...\n",
    "- (18, 47, 56, 57, 58,  1, 15, 47) -> (58)\n",
    "- (n number of examples) -> (element of index n + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623db368-9e88-43a0-873b-3fdc6cb65f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# sampling data\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent dequences we'll  process in parallel\n",
    "block_size = 8 # maximum context length for predictions\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "    \n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa810bb-d511-46ed-857e-62d83fdbd4e6",
   "metadata": {},
   "source": [
    "# First Baseline: Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994d54c3-30e0-4c41-b38f-f97bb62677d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__() # same constructor as the nn.Module class\n",
    "        # each token directly reads off the logies for the nest token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # B, T, C (batch by time by channel, in this case 4, 8, 65)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Pytorch only accepts as the input as a B, C, T so we have to rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of inde=ices in the current context\n",
    "        for _ in range (max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on last time step because it's the 9th element and a prediction of what comes after the 8 characters\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "            \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate( idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539726c-d852-41a3-8a67-0ee4d63c5f20",
   "metadata": {},
   "source": [
    "### Training the Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83e6d0d0-8994-46d5-af78-09daa750422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "\n",
    "# in makemore, we always used SDG as the optimizer, but Adam is better going forward\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # typical good learning rate is 3e-4 but for smaller networks we can get away with a ;arger rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052da199-35de-4671-be43-3a40e34492e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eb2f7b0-0591-4685-8401-c9955b9b3782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate( idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396d048-dcdf-4fc6-a1f5-197694ad00ca",
   "metadata": {},
   "source": [
    "# Single-Head Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ed6ab-1082-4c73-92c2-803cfcff942d",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention\n",
    "\n",
    "We want to couple the 8 tokes in each batch in a way that allows each token to have a context of its preceding tokens. The easiest and most simple way to do this would be to take the average of each every preceding token, which would act as a feature vector for that token. Obviously, a lot of context would be lost here, but this is the first step to building a self-attention block.\n",
    "\n",
    "![title](singeHeadAttention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a5e59ec-aa60-433f-931d-91e15189fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7467458-c351-486a-9a8b-5c396ef20a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "# bag of words model\n",
    "xbow = torch.zeros(B,T,C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # gives us all of the elements from 0 to the T'th element\n",
    "        xbow[b,t] = torch.mean(xprev,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89773333-8bc8-4c1a-a471-79a44210636e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26814c2c-a264-45ea-9b8d-af7312de07c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43b04b30-8cc2-4f23-8f45-7510861feccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b= tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3)) # returns the bottom left triangular part of a matrix, which allows us to do a running average\n",
    "a = a / torch.sum(a, 1, keepdim=True) # sums the matrix so that the rows all add up to 1\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=', a)\n",
    "print(\"b=\", b)\n",
    "print('c', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecb0ef-dbe2-46e3-8be9-49a56ca70ba6",
   "metadata": {},
   "source": [
    "Notice how the first row is equal, and then every row after that is different - that is because they are vertical averages of the preceding elements. This is a convenient way of doing what we did above. Now we'll implement it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c64e01d-08d2-433c-b0f8-f4e0f0130bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 1\n",
    "\n",
    "xbow = torch.zeros(B,T,C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # gives us all of the elements from 0 to the T'th element\n",
    "        xbow[b,t] = torch.mean(xprev,0)\n",
    "\n",
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627abc45-71d5-44f9-a78c-098957084cdc",
   "metadata": {},
   "source": [
    "In version 2, each element has an associated weight to it, showing how relevant it is to predict the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d520428a-934f-4537-87ea-52abf8215f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) --> (B, T, C) - in each element of the batch, a (T, T) @ (T, C) operation is happening, as shown above\n",
    "torch.allclose(xbow, xbow2)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00deb3bc-ff3d-4600-b911-d6074c744e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # instead of 1s and 0s, we're using 0s and -inf to make the future tokens completely irrelevant\n",
    "wei = F.softmax(wei, dim=-1) # applying softmax on -inf will just return 0s and take an average of each row\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8b4b6-7fc8-4f73-b36f-c4f566c33432",
   "metadata": {},
   "source": [
    "The elements from the lower triangular part tell you how relevant each element is for the next token and it increases as you traverse down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808533b0-46e3-4752-bcbc-c8e2123e6131",
   "metadata": {},
   "source": [
    "### Explanation of Self-Attention\n",
    "\n",
    "Each vector has a key and a query. The key is what the token contains and the query is what the token is looking for. To ge the affinities of all the tokens and to understand how significant they are, we take the dot product of the key of any given token and the queries of all previous tokens. This will give us context on how important every other token is to the given token. If a given key and query are specifically aligned, it will have a higher dot product result, which will allow the token to learn more about that other token.\n",
    "\n",
    "The masked fill is used to block out future tokens while performing dot product and the softmax is used to normale the results, as there weill be some negative affinities, so we exponentiate them to get rid of them.\n",
    "\n",
    "In addition, we have a value v because rather than aggregating the values in x directly, applying another linear layer on it and storing the values in v acts as a private value for the signle head. Essentially, it stores the dat that it will communicate to previous tokens if it finds them interesting.\n",
    "\n",
    "**Detailed Explanation**\n",
    "\n",
    "Values in Self-Attention\n",
    "In self-attention, values (v) represent the information that each token or node will communicate to other tokens, based on their affinities.\n",
    "- Think of each token as having private information stored in a vector 'x'.\n",
    "- For the purposes of self-attention, each token produces three vectors: keys (k), queries (q), and values (v).\n",
    "- Queries represent what a token is looking for, keys represent what a token contains, and values represent what a token will share.\n",
    "- Affinities between tokens are calculated by taking the dot product of a query with all the keys.\n",
    "- These affinities determine how much information from each token's value vector will be aggregated into the output of the self-attention mechanism.\n",
    "Here's a breakdown of the process:\n",
    "1. Each token produces keys, queries, and values by applying linear transformations to its private information vector 'x'.\n",
    "2. The query of each token is dot-producted with the keys of all other tokens, resulting in a matrix of affinities or weights ('way').\n",
    "3. This 'way' matrix undergoes masking and softmax to create a normalised distribution representing how much information each token should aggregate from others.\n",
    "4. The values of each token are then aggregated using the weighted sum determined by the normalised 'way' matrix.\n",
    "Therefore, values (v) play a crucial role in determining the information flow in self-attention. They represent the specific information that each token contributes to the communication process, which ultimately influences the output of the self-attention mechanism.\n",
    "\n",
    "**Q&A**\n",
    "\n",
    "Why the Dot Product for Affinities?\n",
    "\n",
    "The dot product is specifically used to calculate affinities in self-attention due to its ability to represent alignment and similarity between vectors. This mathematical property makes it well-suited for determining how much one token should \"pay attention\" to another.\n",
    "While other operations might be mathematically valid, they don't necessarily capture the desired relationship between keys and queries. For example:\n",
    "- Element-wise multiplication: This would only consider the correspondence between individual elements of the vectors, ignoring the overall relationship between them. It wouldn't reflect the idea of a query searching for information represented by a key.\n",
    "- Euclidean distance: This measures the distance between two vectors, with a smaller distance indicating greater similarity. However, in self-attention, we're interested in the degree of alignment or matching, not just proximity. A query and a key could be close in Euclidean space without necessarily being aligned in the direction that represents the desired information exchange.\n",
    "\n",
    "The dot product, on the other hand, directly quantifies the alignment between two vectors. It's sensitive to both the magnitude and direction of the vectors, capturing the idea that a query is seeking specific information represented by the key's direction.\n",
    "Imagine a simplified scenario where a query seeks information about vowels. This query might be represented by a vector pointing in a specific direction associated with vowel characteristics. Keys from vowel tokens would also have vectors pointing in similar directions.\n",
    "The dot product between the query and a key from a vowel token would be high due to their strong alignment. Conversely, keys from consonant tokens would have vectors pointing in different directions, resulting in lower dot products with the vowel query.\n",
    "It's important to remember that the model learns to generate keys and queries that yield meaningful dot products during training. Initially, the linear transformations applied to 'x' to produce keys and queries might be random, but the training process adjusts their weights to capture relevant relationships.\n",
    "This learning process essentially refines the dot product's ability to represent affinities that align with the desired task, making it a powerful tool for data-dependent information flow in self-attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5bd2bc4-7589-4868-93bb-ce6c15bfd596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single Head performing self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False) # although x is (4,8,32), applying a linear layer reshapes it to (32,32), since the total # of elements stays the same\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1) # flipping around the last two dimensions\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # instead of 1s and 0s, we're using 0s and -inf to make the future tokens completely irrelevant\n",
    "wei = F.softmax(wei, dim=-1) # applying softmax on -inf will just return 0s and take an average of each row\n",
    "\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f55861e-5b39-4ddf-b385-a772866172ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a15a6bc-a73e-47d0-abb5-99a4a72fa20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11480f-6f8c-4ab3-8422-2835a44726f7",
   "metadata": {},
   "source": [
    "Few notes about transformers:\n",
    "\n",
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af137c90-27de-4b33-9899-64dd74d9a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = key(x)\n",
    "q = query(x)\n",
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8fc7f-de5a-4067-898f-253e4377fd76",
   "metadata": {},
   "source": [
    "NOTE: above, we have to multiply wei by 1/sqrt(head_size) to normalize the values because if we don't, the values in wei would become either very positive or very negative and this would cause the softmax to turn the values into essentially one-hot vectors since the softmax would sharpen its outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf32bf1-af00-4938-82ad-74342228f864",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "Performing multiple single head attentions in parallel and concatenating them. **Check VSCode for the code.**\n",
    "\n",
    "![title](multiHeadAttention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ed8ac-1d1c-47b8-92cb-0c8230b6e78e",
   "metadata": {},
   "source": [
    "# Feedforward Layer\n",
    "\n",
    "We add a normal linear layer, followed by a non linearity to perform the computation. The attention mechanisms allow the tokens to communicate and assign relative weights for how important they are, but the linear layer is needed to allow them to train on that data and comptute values. \n",
    "\n",
    "<!-- ![title](transformer.webp) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c6d0c-fa13-4bac-a522-317c14eaabb0",
   "metadata": {},
   "source": [
    "# Attention Blocks\n",
    "\n",
    " We'll now implement the attention blocks as shown in the diagram below to speed up the computation.\n",
    "\n",
    "<div>\n",
    "<img src=\"transformer.webp\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3cf950-48cf-45c6-beae-7fa86515388a",
   "metadata": {},
   "source": [
    "### Residual Connections\n",
    "\n",
    "The arrows in the diagram above, skipping the multi head attention blocks and directly going to the add & norm blocks are called residual connections. These essentially act as shown in the diagram below, where the gradients of the output initially channel all the way back to the input, since addition propogates the gradients equally. The blocks on the side aren't intially there, but they slowly start to build gradients over time, which helps with optimization.\n",
    "![title](residualConnections.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04605b76-f0a9-4a72-bcda-d093e0235671",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "Same as batch norm, but across the layer, instead of the batch. Therefore, we delete the buffers and the training conditions.\n",
    "\n",
    "NOTE: In the diagram, addition and normalization are performed AFTER the multi-headed attention block, but in recent years, its become more common to use layernorm right before the attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d49dfa1-6614-410c-a779-8e858aaba3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BatchNorm1d: # modeled after torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1): # \n",
    "\n",
    "        # saving the values\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim) # gain \n",
    "        self.beta = torch.zeros(dim) # bias\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the fwd pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True, unbiased=True) # batch variance\n",
    "        # change the dimensions above in xmean and xvar from 0 to 1\n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance (this is the formula used in PyTorch)\n",
    "        self.out = self.gamma * xhat + self.beta \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100)\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48d0af-02dc-46b2-b425-7903255f4784",
   "metadata": {},
   "source": [
    "# Final Notes\n",
    "\n",
    "<div>\n",
    "<img src=\"transformer.webp\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Encoder vs. Decoder:\n",
    "If you observe the image above, there's a block to the left of the block we just implemented, called the encoder block. What we implemented right now is the decoder block. The encoder block is used only when there's tasks related to translation involved - it encodes the tokens from one language into vectors and the decoder block decodes those vectors in another language.\n",
    "\n",
    "The arrow connecting the two blocks is used to perform **cross-attention**. What makes the decoder a decoder is the triangular mask, which is used for language modeling. The encoder has a transformer applied over the text, but without a triangular mask, which allow all of the tokens to talk to each other as much as they want. This context is then fed into the decoder. The decoder is still trained over whatever text we want it to train on to output coherent text, but it has added context with the embeddings from the other language.\n",
    "\n",
    "To put in transformer terms, the queries are still coming from x, but there's additional keys and values are coming from the encoder output, feeding into every single block of the decoder. \n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5870964-5750-466e-8eec-eb6410248698",
   "metadata": {},
   "source": [
    "# EXERCISES:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9945af-6a97-4ec0-bc14-b5dda3ead32e",
   "metadata": {},
   "source": [
    "1. The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
    "\n",
    "### Don't understand fully come back to later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20dc2dce-7d77-4665-a02a-36146f2eda57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 48])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C,N = 4,8,32,3\n",
    "x = torch.randn(B,T,C,N)\n",
    "\n",
    "# single Head performing self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C*N, head_size*3, bias=False)\n",
    "query = nn.Linear(C*N, head_size*3, bias=False)\n",
    "value = nn.Linear(C*N, head_size*3, bias=False) # although x is (4,8,32), applying a linear layer reshapes it to (32,32), since the total # of elements stays the same\n",
    "k = key(x.view(32,96))\n",
    "q = query(x.view(32,96))\n",
    "# (B, T, 48) @ (B, 48, T) ---> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1) # flipping around the last two dimensions\n",
    "\n",
    "tril = torch.tril(torch.ones(32, 32))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # instead of 1s and 0s, we're using 0s and -inf to make the future tokens completely irrelevant\n",
    "wei = F.softmax(wei, dim=-1) # applying softmax on -inf will just return 0s and take an average of each row\n",
    "\n",
    "\n",
    "v = value(x.view(32,96))\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c46b28f0-e253-447c-9464-76b26c5f65e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 8, 4, 8]' is invalid for input of size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m q \u001b[38;5;241m=\u001b[39m query(x) \u001b[38;5;66;03m# -> (4, 8, 16)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(q\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, nh, T, hs)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(B, T, n_head, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_head)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, nh, T, hs)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(B, T, n_head, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_head)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, nh, T, hs)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 8, 4, 8]' is invalid for input of size 512"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "\n",
    "# C\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single Head performing self-attention\n",
    "n_head = 4\n",
    "# head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # -> 32, 16\n",
    "query = nn.Linear(C, head_size, bias=False) # -> 32, 16\n",
    "value = nn.Linear(C, head_size, bias=False) # although x is (4,8,32), applying a linear layer reshapes it to (32,32), since the total # of elements stays the same\n",
    "k = key(x) # -> (4, 8, 16)\n",
    "q = query(x) # -> (4, 8, 16)\n",
    "print(q.shape)\n",
    "k = k.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "o90q = q.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "v = v.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1) # flipping around the last two dimensions\n",
    "\n",
    "tril = torch.tril(torch.ones(n_head, C // n_head))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # instead of 1s and 0s, we're using 0s and -inf to make the future tokens completely irrelevant\n",
    "wei = F.softmax(wei, dim=-1) # applying softmax on -inf will just return 0s and take an average of each row\n",
    "\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52dcb348-e807-49f8-8633-79f1289590d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.1871e-03, 9.9881e-01, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.1622e-01, 1.2857e-01, 7.5521e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [9.0579e-02, 1.9958e-03, 2.9385e-04,  ..., 9.5752e-04, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.5396e-03, 1.3047e-03, 4.8368e-03,  ..., 9.5784e-02, 1.7049e-03,\n",
       "         0.0000e+00],\n",
       "        [2.5644e-03, 1.9540e-05, 4.1811e-03,  ..., 3.7376e-05, 1.7229e-03,\n",
       "         3.0560e-05]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
